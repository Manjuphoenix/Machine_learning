{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Manjuphoenix/Machine_learning/blob/master/notebooks/how-to-segment-anything-with-sam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "HOME = os.getcwd()\n",
        "print(\"HOME:\", HOME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgS8jFPMnj5h",
        "outputId": "22c2e386-4a8a-4e5f-d868-bb4b75374715"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HOME: /content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Segment Anything Model (SAM) and other dependencies"
      ],
      "metadata": {
        "id": "YN3DPGZSn57p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {HOME}\n",
        "\n",
        "import sys\n",
        "!{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1H9YruJen0Q8",
        "outputId": "2bf52457-3165-441b-8573-a7d4ec891df0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/facebookresearch/segment-anything.git\n",
            "  Cloning https://github.com/facebookresearch/segment-anything.git to /tmp/pip-req-build-i5pv2ak9\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything.git /tmp/pip-req-build-i5pv2ak9\n",
            "  Resolved https://github.com/facebookresearch/segment-anything.git to commit 6fdee8f2727f4506cfbbe553e23b895e27956588\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: segment-anything\n",
            "  Building wheel for segment-anything (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segment-anything: filename=segment_anything-1.0-py3-none-any.whl size=36589 sha256=946041d2d8e11bc7014b9023a77685089c7f0b33a792c432cb9db3b0288b795a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-xvsgy25a/wheels/10/cf/59/9ccb2f0a1bcc81d4fbd0e501680b5d088d690c6cfbc02dc99d\n",
            "Successfully built segment-anything\n",
            "Installing collected packages: segment-anything\n",
            "Successfully installed segment-anything-1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q jupyter_bbox_widget roboflow dataclasses-json supervision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3CtzYroC2Lb",
        "outputId": "773de826-8278-4e37-8c80-f200e811a695"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m367.8/367.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download SAM weights"
      ],
      "metadata": {
        "id": "2VeYIWh1iDWW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {HOME}\n",
        "!mkdir {HOME}/weights\n",
        "%cd {HOME}/weights\n",
        "\n",
        "!wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aszw1OxBwowI",
        "outputId": "8d103ee4-2c8a-405b-812a-ff8e51bb47eb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/weights\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "CHECKPOINT_PATH = os.path.join(HOME, \"weights\", \"sam_vit_h_4b8939.pth\")\n",
        "print(CHECKPOINT_PATH, \"; exist:\", os.path.isfile(CHECKPOINT_PATH))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxoFmhsHw_fG",
        "outputId": "3c530a0b-d102-46a4-e3aa-24b8894a54ed"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/weights/sam_vit_h_4b8939.pth ; exist: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Model"
      ],
      "metadata": {
        "id": "vlhbd_f4xfiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "MODEL_TYPE = \"vit_h\""
      ],
      "metadata": {
        "id": "t6_9PSZupghA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
        "\n",
        "sam = sam_model_registry[MODEL_TYPE](checkpoint=CHECKPOINT_PATH).to(device=DEVICE)"
      ],
      "metadata": {
        "id": "n41g6y-Zx-9x"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Automated Mask Generation\n",
        "\n",
        "To run automatic mask generation, provide a SAM model to the `SamAutomaticMaskGenerator` class. Set the path below to the SAM checkpoint. Running on CUDA and with the default model is recommended."
      ],
      "metadata": {
        "id": "pi3C4uDWo10h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mask_generator = SamAutomaticMaskGenerator(sam)"
      ],
      "metadata": {
        "id": "CtymFaiKyQ57"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple, Union, Optional\n",
        "from dataclasses_json import dataclass_json\n",
        "from supervision import Detections\n",
        "\n",
        "\n",
        "@dataclass_json\n",
        "@dataclass\n",
        "class COCOCategory:\n",
        "    id: int\n",
        "    name: str\n",
        "    supercategory: str\n",
        "\n",
        "\n",
        "@dataclass_json\n",
        "@dataclass\n",
        "class COCOImage:\n",
        "    id: int\n",
        "    width: int\n",
        "    height: int\n",
        "    file_name: str\n",
        "    license: int\n",
        "    date_captured: str\n",
        "    coco_url: Optional[str] = None\n",
        "    flickr_url: Optional[str] = None\n",
        "\n",
        "\n",
        "@dataclass_json\n",
        "@dataclass\n",
        "class COCOAnnotation:\n",
        "    id: int\n",
        "    image_id: int\n",
        "    category_id: int\n",
        "    segmentation: List[List[float]]\n",
        "    area: float\n",
        "    bbox: Tuple[float, float, float, float]\n",
        "    iscrowd: int\n",
        "\n",
        "\n",
        "@dataclass_json\n",
        "@dataclass\n",
        "class COCOLicense:\n",
        "    id: int\n",
        "    name: str\n",
        "    url: str\n",
        "\n",
        "\n",
        "@dataclass_json\n",
        "@dataclass\n",
        "class COCOJson:\n",
        "    images: List[COCOImage]\n",
        "    annotations: List[COCOAnnotation]\n",
        "    categories: List[COCOCategory]\n",
        "    licenses: List[COCOLicense]\n",
        "\n",
        "\n",
        "def load_coco_json(json_file: str) -> COCOJson:\n",
        "    import json\n",
        "\n",
        "    with open(json_file, \"r\") as f:\n",
        "        json_data = json.load(f)\n",
        "\n",
        "    return COCOJson.from_dict(json_data)\n",
        "\n",
        "\n",
        "class COCOJsonUtility:\n",
        "    @staticmethod\n",
        "    def get_annotations_by_image_id(coco_data: COCOJson, image_id: int) -> List[COCOAnnotation]:\n",
        "        return [annotation for annotation in coco_data.annotations if annotation.image_id == image_id]\n",
        "\n",
        "    @staticmethod\n",
        "    def get_annotations_by_image_path(coco_data: COCOJson, image_path: str) -> Optional[List[COCOAnnotation]]:\n",
        "        image = COCOJsonUtility.get_image_by_path(coco_data, image_path)\n",
        "        if image:\n",
        "            return COCOJsonUtility.get_annotations_by_image_id(coco_data, image.id)\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    @staticmethod\n",
        "    def get_image_by_path(coco_data: COCOJson, image_path: str) -> Optional[COCOImage]:\n",
        "        for image in coco_data.images:\n",
        "            if image.file_name == image_path:\n",
        "                return image\n",
        "        return None\n",
        "\n",
        "    @staticmethod\n",
        "    def annotations2detections(annotations: List[COCOAnnotation]) -> Detections:\n",
        "        class_id, xyxy = [], []\n",
        "\n",
        "        for annotation in annotations:\n",
        "            x_min, y_min, width, height = annotation.bbox\n",
        "            class_id.append(annotation.category_id)\n",
        "            xyxy.append([\n",
        "                x_min,\n",
        "                y_min,\n",
        "                x_min + width,\n",
        "                y_min + height\n",
        "            ])\n",
        "\n",
        "        return Detections(\n",
        "            xyxy=np.array(xyxy, dtype=int),\n",
        "            class_id=np.array(class_id, dtype=int)\n",
        "        )"
      ],
      "metadata": {
        "id": "dZSU9BpHr2gc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download Dataset from Roboflow"
      ],
      "metadata": {
        "id": "W-jQ5c4TQAic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {HOME}\n",
        "\n",
        "import roboflow\n",
        "from roboflow import Roboflow\n",
        "\n",
        "roboflow.login()\n",
        "\n",
        "rf = Roboflow()\n",
        "\n",
        "project = rf.workspace(\"hashira-fhxpj\").project(\"mri-brain-tumor\")\n",
        "dataset = project.version(1).download(\"coco\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxH9V2nHQC4M",
        "outputId": "cc1146b8-4816-4b9e-edb8-5c366026b82c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "\rvisit https://app.roboflow.com/auth-cli to get your authentication token.\n",
            "Paste the authentication token here: ··········\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Downloading Dataset Version Zip in MRI-BRAIN-TUMOR-1 to coco: 100% [1561113 / 1561113] bytes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Dataset Version Zip to MRI-BRAIN-TUMOR-1 in coco:: 100%|██████████| 85/85 [00:00<00:00, 2992.34it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "DATA_SET_SUBDIRECTORY = \"test\"\n",
        "ANNOTATIONS_FILE_NAME = \"_annotations.coco.json\"\n",
        "IMAGES_DIRECTORY_PATH = os.path.join(dataset.location, DATA_SET_SUBDIRECTORY)\n",
        "ANNOTATIONS_FILE_PATH = os.path.join(dataset.location, DATA_SET_SUBDIRECTORY, ANNOTATIONS_FILE_NAME)"
      ],
      "metadata": {
        "id": "N-Xi4Dx0QuRL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coco_data = load_coco_json(json_file=ANNOTATIONS_FILE_PATH)\n",
        "\n",
        "CLASSES = [\n",
        "    category.name\n",
        "    for category\n",
        "    in coco_data.categories\n",
        "    if category.supercategory != 'none'\n",
        "]\n",
        "\n",
        "IMAGES = [\n",
        "    image.file_name\n",
        "    for image\n",
        "    in coco_data.images\n",
        "]"
      ],
      "metadata": {
        "id": "a8b3oKP0QyiB"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CLASSES"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFQwR9-aQ0kx",
        "outputId": "eb84c5b1-4683-4d56-8e7c-00673de1cc4f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['YES-TUMOR']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Single Image Bounding Box to Mask"
      ],
      "metadata": {
        "id": "s3WqT5qTQ6tI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set random seed to allow easy reproduction of the experiment\n",
        "\n",
        "import random\n",
        "random.seed(10)"
      ],
      "metadata": {
        "id": "MuUKYCLnRAaN"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask_predictor = SamPredictor(sam)"
      ],
      "metadata": {
        "id": "5_78Qpvi9vUn"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import supervision as sv\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "import PIL\n",
        "\n",
        "\n",
        "\n",
        "# EXAMPLE_IMAGE_NAME = random.choice(IMAGES)\n",
        "EXAMPLE_IMAGE_NAME = \"Y16_JPG.rf.96e3fc38718c5f64776810040e63363c.jpg\"\n",
        "EXAMPLE_IMAGE_PATH = os.path.join(dataset.location, DATA_SET_SUBDIRECTORY, EXAMPLE_IMAGE_NAME)\n",
        "\n",
        "\n",
        "print(\"File name: \", EXAMPLE_IMAGE_NAME)\n",
        "# load dataset annotations\n",
        "annotations = COCOJsonUtility.get_annotations_by_image_path(coco_data=coco_data, image_path=EXAMPLE_IMAGE_NAME)\n",
        "ground_truth = COCOJsonUtility.annotations2detections(annotations=annotations)\n",
        "\n",
        "# small hack - coco numerate classes from 1, model from 0 + we drop first redundant class from coco json\n",
        "ground_truth.class_id = ground_truth.class_id - 1\n",
        "\n",
        "# load image\n",
        "image_bgr = cv2.imread(EXAMPLE_IMAGE_PATH)\n",
        "image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# initiate annotator\n",
        "box_annotator = sv.BoxAnnotator(color=sv.Color.red())\n",
        "mask_annotator = sv.MaskAnnotator(color=sv.Color.red())\n",
        "\n",
        "# annotate ground truth\n",
        "annotated_frame_ground_truth = box_annotator.annotate(scene=image_bgr.copy(), detections=ground_truth, skip_label=True)\n",
        "\n",
        "output = []\n",
        "img_shp = image_rgb.shape\n",
        "img = np.zeros(img_shp)\n",
        "# run SAM inference\n",
        "mask_predictor.set_image(image_rgb)\n",
        "\n",
        "for i in ground_truth.xyxy:\n",
        "  masks, scores, logits = mask_predictor.predict(\n",
        "      box=i,\n",
        "      multimask_output=True\n",
        "  )\n",
        "\n",
        "  detections = sv.Detections(\n",
        "      xyxy=sv.mask_to_xyxy(masks=masks),\n",
        "      mask=masks\n",
        "  )\n",
        "  # detections = detections[detections.area == np.max(detections.area)]\n",
        "  detections = detections[detections.area == np.max(detections.area)]\n",
        "  # output += detections.mask\n",
        "  print( \"masks__________________-------------\",detections.mask,)\n",
        "  print(type(detections))\n",
        "\n",
        "  # annotated_image = mask_annotator.annotate(scene=image_bgr.copy(), detections=detections)\n",
        "\n",
        "  ################------Old code-----------################\n",
        "  # annotated_image = mask_annotator.annotate(scene=image_bgr.copy(), detections=detections)\n",
        "\n",
        "  # sv.plot_images_grid(\n",
        "  #       images=[annotated_frame_ground_truth, annotated_image],\n",
        "  #       grid_size=(1, 2),\n",
        "  #       titles=['source image', 'segmented image']\n",
        "  # )\n",
        "  ##############----------end----------------###############\n",
        "\n",
        "  output.append(detections)\n",
        "  print(len(output))\n",
        "  print(output[0].mask)\n",
        "\n",
        "  for j in output:\n",
        "    annotated_image = mask_annotator.annotate(scene=image_bgr.copy(), detections=j[0:2])\n",
        "    print(annotated_image.shape, \"--------------------\")\n",
        "    img += annotated_image\n",
        "    img = img.reshape(416,416,1)\n",
        "\n",
        "    plt.imshow(img)\n",
        "\n",
        "\n",
        "    sv.plot_images_grid(\n",
        "          images=[annotated_frame_ground_truth, annotated_image],\n",
        "          grid_size=(1, 2),\n",
        "          titles=['source image', 'segmented image']\n",
        "    )\n",
        "    print(j[0].mask.shape)\n",
        "    imgplot = plt.imshow(j[0].mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 549
        },
        "id": "RHw4yH8XRCo9",
        "outputId": "fa995e10-b77e-478f-ebd6-87186f795aae"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File name:  Y16_JPG.rf.96e3fc38718c5f64776810040e63363c.jpg\n",
            "masks__________________------------- [[[False False False ... False False False]\n",
            "  [False False False ... False False False]\n",
            "  [False False False ... False False False]\n",
            "  ...\n",
            "  [False False False ... False False False]\n",
            "  [False False False ... False False False]\n",
            "  [False False False ... False False False]]]\n",
            "<class 'supervision.detection.core.Detections'>\n",
            "1\n",
            "[[[False False False ... False False False]\n",
            "  [False False False ... False False False]\n",
            "  [False False False ... False False False]\n",
            "  ...\n",
            "  [False False False ... False False False]\n",
            "  [False False False ... False False False]\n",
            "  [False False False ... False False False]]]\n",
            "(416, 416, 3) --------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-98-b620c78b40f5>\u001b[0m in \u001b[0;36m<cell line: 39>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotated_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"--------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mannotated_image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m416\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m416\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 519168 into shape (416,416,1)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mGWIXYer820E"
      },
      "execution_count": 42,
      "outputs": []
    }
  ]
}